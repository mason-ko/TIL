"""
Langfuse Step 1: ê¸°ë³¸ ì„¤ì • ë° Ollama ì—°ë™

ì‹¤í–‰ ì „ í•„ìš”ì‚¬í•­:
1. Langfuse ì„œë²„ ì‹¤í–‰:
   docker compose up -d

2. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
   ollama pull llama3

3. .env íŒŒì¼ ì„¤ì •:
   LANGFUSE_PUBLIC_KEY=pk-lf-...
   LANGFUSE_SECRET_KEY=sk-lf-...
   LANGFUSE_HOST=http://localhost:3000
"""

import os
from dotenv import load_dotenv
from langfuse import Langfuse
from langfuse.callback import CallbackHandler
from langfuse.decorators import observe, langfuse_context
from langchain_community.llms import Ollama
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

load_dotenv()


def example1_basic_tracing():
    """
    ì˜ˆì œ 1: ê¸°ë³¸ íŠ¸ë ˆì´ì‹±

    Ollama + Langfuseì˜ ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤.
    """
    print("=== ì˜ˆì œ 1: ê¸°ë³¸ íŠ¸ë ˆì´ì‹± (Ollama) ===\n")

    # Langfuse ì´ˆê¸°í™”
    langfuse = Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_HOST", "http://localhost:3000")
    )

    # Ollama ëª¨ë¸
    llm = Ollama(model="llama3")

    # íŠ¸ë ˆì´ì‹± ì‹œì‘
    trace = langfuse.trace(name="basic-ollama-call")

    # LLM í˜¸ì¶œ
    question = "Langfuseë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    print(f"ì§ˆë¬¸: {question}")

    generation = trace.generation(
        name="ollama-generation",
        model="llama3",
        input=question
    )

    response = llm.invoke(question)

    generation.end(output=response)

    print(f"ì‘ë‹µ: {response}\n")
    print(f"âœ… Langfuse UIì—ì„œ í™•ì¸: {trace.get_trace_url()}\n")


def example2_langchain_integration():
    """
    ì˜ˆì œ 2: LangChain í†µí•©

    LangChainì˜ CallbackHandlerë¥¼ ì‚¬ìš©í•˜ì—¬
    ìë™ìœ¼ë¡œ íŠ¸ë ˆì´ì‹±í•©ë‹ˆë‹¤.
    """
    print("=== ì˜ˆì œ 2: LangChain í†µí•© ===\n")

    # Langfuse Callback Handler
    langfuse_handler = CallbackHandler(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_HOST")
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = PromptTemplate(
        input_variables=["topic"],
        template="ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ 3ì¤„ ì´ë‚´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {topic}"
    )

    llm = Ollama(model="llama3")
    chain = LLMChain(llm=llm, prompt=prompt)

    # ì²´ì¸ ì‹¤í–‰ (ìë™ íŠ¸ë ˆì´ì‹±)
    topic = "Vector Database"
    print(f"ì£¼ì œ: {topic}")

    response = chain.invoke(
        {"topic": topic},
        config={"callbacks": [langfuse_handler]}
    )

    print(f"ì‘ë‹µ: {response['text']}\n")
    print("âœ… Langfuse UIì—ì„œ ì²´ì¸ì˜ ê° ë‹¨ê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n")


@observe()
def translate_text(text: str, target_lang: str) -> str:
    """
    ì˜ˆì œ 3ì—ì„œ ì‚¬ìš©í•  ë²ˆì—­ í•¨ìˆ˜

    @observe ë°ì½”ë ˆì´í„°ë¡œ ìë™ íŠ¸ë ˆì´ì‹±
    """
    llm = Ollama(model="llama3")

    # í˜„ì¬ traceì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    langfuse_context.update_current_trace(
        metadata={
            "source_text": text,
            "target_language": target_lang,
            "model": "llama3",
            "environment": "development"
        },
        tags=["translation", "ollama", "step1"]
    )

    prompt = f"Translate the following text to {target_lang}. Only provide the translation, no explanations: {text}"
    response = llm.invoke(prompt)

    return response


def example3_metadata():
    """
    ì˜ˆì œ 3: ë©”íƒ€ë°ì´í„° ì¶”ê°€

    @observe ë°ì½”ë ˆì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬
    ë” ìƒì„¸í•œ ì •ë³´ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    print("=== ì˜ˆì œ 3: ë©”íƒ€ë°ì´í„° ì¶”ê°€ ===\n")

    text = "Hello, world!"
    target = "Korean"

    print(f"ì›ë¬¸: {text}")
    print(f"ëª©í‘œ ì–¸ì–´: {target}")

    result = translate_text(text, target)

    print(f"ë²ˆì—­ ê²°ê³¼: {result}\n")
    print("âœ… Langfuse UIì—ì„œ ë©”íƒ€ë°ì´í„°ì™€ íƒœê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n")


def example4_error_tracking():
    """
    ì˜ˆì œ 4: ì—ëŸ¬ ì¶”ì 

    ì—ëŸ¬ê°€ ë°œìƒí•´ë„ Langfuseì— ê¸°ë¡ë©ë‹ˆë‹¤.
    """
    print("=== ì˜ˆì œ 4: ì—ëŸ¬ ì¶”ì  ===\n")

    langfuse = Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_HOST", "http://localhost:3000")
    )

    trace = langfuse.trace(name="error-example")

    try:
        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í˜¸ì¶œ (ì—ëŸ¬ ë°œìƒ)
        llm = Ollama(model="nonexistent-model")

        generation = trace.generation(
            name="will-fail",
            model="nonexistent-model",
            input="This will fail"
        )

        response = llm.invoke("This will fail")
        generation.end(output=response)

    except Exception as e:
        # ì—ëŸ¬ ì •ë³´ ê¸°ë¡
        generation.end(
            output=None,
            metadata={"error": str(e), "error_type": type(e).__name__},
            level="ERROR"
        )

        print(f"âŒ ì˜ë„ì  ì—ëŸ¬ ë°œìƒ: {e}")
        print(f"âœ… Langfuseì—ì„œ ì—ëŸ¬ ì¶”ì  í™•ì¸: {trace.get_trace_url()}\n")


def example5_multiple_generations():
    """
    ì˜ˆì œ 5: ì—¬ëŸ¬ ë²ˆì˜ LLM í˜¸ì¶œ

    í•˜ë‚˜ì˜ trace ì•ˆì—ì„œ ì—¬ëŸ¬ ë²ˆ LLMì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    print("=== ì˜ˆì œ 5: ì—¬ëŸ¬ LLM í˜¸ì¶œ ===\n")

    langfuse = Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_HOST", "http://localhost:3000")
    )

    llm = Ollama(model="llama3")

    # í•˜ë‚˜ì˜ trace
    trace = langfuse.trace(name="multi-step-conversation")

    # 1ë‹¨ê³„: ì£¼ì œ ì„ ì •
    gen1 = trace.generation(name="step1-topic", model="llama3", input="AI ê¸°ìˆ  í•˜ë‚˜ë§Œ ë§í•´ì¤˜")
    topic = llm.invoke("AI ê¸°ìˆ  í•˜ë‚˜ë§Œ ë§í•´ì¤˜")
    gen1.end(output=topic)
    print(f"1ë‹¨ê³„ - ì£¼ì œ: {topic}")

    # 2ë‹¨ê³„: ì„¤ëª… ìš”ì²­
    gen2 = trace.generation(name="step2-explain", model="llama3", input=f"{topic}ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜")
    explanation = llm.invoke(f"{topic}ì— ëŒ€í•´ 2ì¤„ë¡œ ì„¤ëª…í•´ì¤˜")
    gen2.end(output=explanation)
    print(f"2ë‹¨ê³„ - ì„¤ëª…: {explanation}")

    # 3ë‹¨ê³„: í™œìš© ì‚¬ë¡€
    gen3 = trace.generation(name="step3-usecase", model="llama3", input=f"{topic}ì˜ í™œìš© ì‚¬ë¡€ëŠ”?")
    usecase = llm.invoke(f"{topic}ì˜ í™œìš© ì‚¬ë¡€ í•˜ë‚˜ë§Œ ì•Œë ¤ì¤˜")
    gen3.end(output=usecase)
    print(f"3ë‹¨ê³„ - í™œìš©: {usecase}\n")

    print(f"âœ… 3ë‹¨ê³„ ëŒ€í™”ê°€ í•˜ë‚˜ì˜ traceë¡œ ê¸°ë¡ë¨: {trace.get_trace_url()}\n")


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    required_env = ["LANGFUSE_PUBLIC_KEY", "LANGFUSE_SECRET_KEY", "LANGFUSE_HOST"]
    missing = [env for env in required_env if not os.getenv(env)]

    if missing:
        print("âŒ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print(f"   ëˆ„ë½: {', '.join(missing)}")
        print("\n.env íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
        print("   LANGFUSE_PUBLIC_KEY=pk-lf-...")
        print("   LANGFUSE_SECRET_KEY=sk-lf-...")
        print("   LANGFUSE_HOST=http://localhost:3000")
        print("\nê·¸ë¦¬ê³  Langfuse ì„œë²„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("   docker compose up -d")
        exit(1)

    print("ğŸš€ Langfuse Step 1: Ollama ì—°ë™ ë° ê¸°ë³¸ íŠ¸ë ˆì´ì‹±\n")
    print("=" * 60)
    print()

    # ì˜ˆì œ 1: ê¸°ë³¸ íŠ¸ë ˆì´ì‹±
    try:
        example1_basic_tracing()
    except Exception as e:
        print(f"âš ï¸  ì˜ˆì œ 1 ì‹¤íŒ¨ (Ollama ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸): {e}\n")

    print("-" * 60)
    print()

    # ì˜ˆì œ 2: LangChain í†µí•©
    try:
        example2_langchain_integration()
    except Exception as e:
        print(f"âš ï¸  ì˜ˆì œ 2 ì‹¤íŒ¨: {e}\n")

    print("-" * 60)
    print()

    # ì˜ˆì œ 3: ë©”íƒ€ë°ì´í„°
    try:
        example3_metadata()
    except Exception as e:
        print(f"âš ï¸  ì˜ˆì œ 3 ì‹¤íŒ¨: {e}\n")

    print("-" * 60)
    print()

    # ì˜ˆì œ 4: ì—ëŸ¬ ì¶”ì 
    example4_error_tracking()

    print("-" * 60)
    print()

    # ì˜ˆì œ 5: ì—¬ëŸ¬ LLM í˜¸ì¶œ
    try:
        example5_multiple_generations()
    except Exception as e:
        print(f"âš ï¸  ì˜ˆì œ 5 ì‹¤íŒ¨: {e}\n")

    print("=" * 60)
    print()

    print("âœ… ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")
    print()
    print("ğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. http://localhost:3000 ì ‘ì†")
    print("   2. Traces íƒ­ì—ì„œ ëª¨ë“  ì‹¤í–‰ ê¸°ë¡ í™•ì¸")
    print("   3. ê° traceë¥¼ í´ë¦­í•˜ì—¬ ìƒì„¸ ì •ë³´ í™•ì¸")
    print("   4. ì§€ì—°ì‹œê°„, ì…ë ¥/ì¶œë ¥, ë©”íƒ€ë°ì´í„° ë¶„ì„")
    print()
    print("ğŸ“š ë‹¤ìŒ íŠœí† ë¦¬ì–¼: step2.py - RAG íŒŒì´í”„ë¼ì¸ íŠ¸ë ˆì´ì‹±")
