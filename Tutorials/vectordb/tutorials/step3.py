"""
Vector DB Step 3: RAG ì‹¤ì „ êµ¬í˜„

pip install chromadb langchain-community
"""

import chromadb
import os
from dotenv import load_dotenv

load_dotenv()
from langchain_community.llms import Ollama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI

def create_knowledge_base():
    """ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•"""
    documents = [
        """LangGraphëŠ” LangChain íŒ€ì´ ê°œë°œí•œ ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
        ë³µì¡í•œ AI ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ ë•Œ ì‚¬ìš©í•˜ë©°, StateGraphë¥¼ í†µí•´ ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.""",

        """LangfuseëŠ” ì˜¤í”ˆì†ŒìŠ¤ LLM Observability í”Œë«í¼ì…ë‹ˆë‹¤.
        Self-hostedë¡œ ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°, Ollama ê°™ì€ ë¡œì»¬ LLMê³¼ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤.
        ëª¨ë“  LLM í˜¸ì¶œì„ ì¶”ì í•˜ê³  ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""",

        """ChromaDBëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.
        Pythonìœ¼ë¡œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ìë™ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
        RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.""",

        """OllamaëŠ” ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
        Llama 3, Mistral ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì§€ì›í•˜ë©°, ì™„ì „ ë¬´ë£Œì…ë‹ˆë‹¤.
        API ë¹„ìš© ì—†ì´ LLMì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    ]

    # ë¬¸ì„œ ë¶„í•  (ê¸´ ë¬¸ì„œëŠ” ì‘ê²Œ ë‚˜ëˆ”)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=20
    )

    chunks = []
    for doc in documents:
        chunks.extend(splitter.split_text(doc))

    # ChromaDBì— ì €ì¥
    client = chromadb.PersistentClient(path="./rag_db")
    collection = client.get_or_create_collection("knowledge")

    # ê¸°ì¡´ ë°ì´í„° í´ë¦¬ì–´ (í…ŒìŠ¤íŠ¸ìš©)
    if collection.count() > 0:
        collection.delete(ids=[collection.get()['ids'][i] for i in range(collection.count())])

    collection.add(
        documents=chunks,
        ids=[f"chunk_{i}" for i in range(len(chunks))]
    )

    return collection


def rag_query(collection, question):
    """RAG ì§ˆì˜ì‘ë‹µ"""
    print(f"\nì§ˆë¬¸: {question}\n")

    # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    results = collection.query(
        query_texts=[question],
        n_results=3
    )

    context = "\n".join(results['documents'][0])
    print(f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:\n{context}\n")

    # 2. LLMì— ì§ˆë¬¸
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.7,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )

    prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ì •ë³´ì— ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    answer = llm.invoke(prompt)
    print(f"ë‹µë³€: {answer}\n")

    return answer


def main():
    print("ğŸš€ RAG ì‹œìŠ¤í…œ êµ¬ì¶•\n")
    print("=" * 60)

    # ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
    print("\n1. ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
    collection = create_knowledge_base()
    print(f"   âœ… {collection.count()}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")

    # ì§ˆì˜ì‘ë‹µ
    print("\n2. ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    print("-" * 60)

    questions = [
        "ë¡œì»¬ LLMì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë„êµ¬ëŠ”?",
        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë­ì•¼?",
        "ë¹„ìš© ì—†ì´ LLM ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€?"
    ]

    try:
        for q in questions:
            rag_query(collection, q)
            print("-" * 60)
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        print("   GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    print("\nâœ… RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
    print("\nğŸ’¡ í•µì‹¬: ê²€ìƒ‰ â†’ ì»¨í…ìŠ¤íŠ¸ â†’ LLM")
    print("ğŸ“š ë‹¤ìŒ: step4.py - ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ\n")


if __name__ == "__main__":
    main()
