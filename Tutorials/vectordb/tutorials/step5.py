"""
Vector DB Step 5: RAG í†µí•©

LangChainì„ í™œìš©í•œ í”„ë¡œë•ì…˜ê¸‰ RAG ì‹œìŠ¤í…œ êµ¬ì¶•

pip install chromadb langchain-community langchain-core
"""

import chromadb
import os
from dotenv import load_dotenv

load_dotenv()


def setup_knowledge_base():
    """ì§€ì‹ ë² ì´ìŠ¤ ì„¤ì •"""
    client = chromadb.PersistentClient(path="./final_rag_db")

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
    try:
        client.delete_collection("tech_docs")
    except:
        pass

    collection = client.create_collection("tech_docs")

    # ê¸°ìˆ  ë¬¸ì„œ ë°ì´í„°
    documents = [
        "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Chain, Agent, Retriever ë“±ì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë³µì¡í•œ AI ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "Vector DatabaseëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ChromaDB, Pinecone ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ê¸°ë²•ì…ë‹ˆë‹¤. ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤.",
        "OllamaëŠ” ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. Llama 3, Mistral ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.",
        "LangfuseëŠ” ì˜¤í”ˆì†ŒìŠ¤ LLM ëª¨ë‹ˆí„°ë§ í”Œë«í¼ì…ë‹ˆë‹¤. Self-hosted ê°€ëŠ¥í•˜ë©° ë¬´ë£Œì…ë‹ˆë‹¤.",
    ]

    metadatas = [
        {"source": "langchain-docs", "category": "framework"},
        {"source": "langgraph-docs", "category": "framework"},
        {"source": "vectordb-guide", "category": "database"},
        {"source": "rag-tutorial", "category": "technique"},
        {"source": "ollama-docs", "category": "tools"},
        {"source": "langfuse-docs", "category": "monitoring"},
    ]

    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=[f"doc{i}" for i in range(len(documents))]
    )

    return collection


def basic_retrieval(collection, query):
    """ê¸°ë³¸ ê²€ìƒ‰"""
    print(f"\nğŸ” ê²€ìƒ‰: {query}")
    print("-" * 60)

    results = collection.query(
        query_texts=[query],
        n_results=3
    )

    for i, (doc, meta) in enumerate(zip(
        results['documents'][0],
        results['metadatas'][0]
    ), 1):
        print(f"\n   {i}. [{meta['category']}] {meta['source']}")
        print(f"      {doc[:100]}...")


def filtered_retrieval(collection, query, category):
    """ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²€ìƒ‰"""
    print(f"\nğŸ” í•„í„°ë§ ê²€ìƒ‰: {query} (category={category})")
    print("-" * 60)

    results = collection.query(
        query_texts=[query],
        n_results=3,
        where={"category": category}
    )

    if results['documents'][0]:
        for i, doc in enumerate(results['documents'][0], 1):
            print(f"   {i}. {doc[:80]}...")
    else:
        print("   ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")


def mmr_retrieval(collection, query):
    """MMR (ë‹¤ì–‘ì„± ê³ ë ¤) ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜"""
    print(f"\nğŸ” MMR ê²€ìƒ‰ (ë‹¤ì–‘ì„± ê³ ë ¤): {query}")
    print("-" * 60)

    # ë” ë§ì€ í›„ë³´ ê°€ì ¸ì˜¤ê¸°
    results = collection.query(
        query_texts=[query],
        n_results=10
    )

    # ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± í™•ë³´
    seen_categories = set()
    diverse_docs = []

    for doc, meta in zip(
        results['documents'][0],
        results['metadatas'][0]
    ):
        category = meta['category']
        if category not in seen_categories:
            diverse_docs.append((doc, category))
            seen_categories.add(category)

        if len(diverse_docs) >= 3:
            break

    print("   ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œ ì„ íƒ:")
    for i, (doc, cat) in enumerate(diverse_docs, 1):
        print(f"   {i}. [{cat}] {doc[:80]}...")


def rag_with_context(collection, question):
    """ì»¨í…ìŠ¤íŠ¸ í¬í•¨ RAG"""
    print(f"\nğŸ’¬ RAG ì§ˆì˜ì‘ë‹µ: {question}")
    print("=" * 60)

    # 1. ê²€ìƒ‰
    results = collection.query(
        query_texts=[question],
        n_results=2
    )

    context_parts = []
    for doc, meta in zip(
        results['documents'][0],
        results['metadatas'][0]
    ):
        context_parts.append(f"[ì¶œì²˜: {meta['source']}]\n{doc}")

    context = "\n\n".join(context_parts)

    print("\nğŸ“„ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:")
    print("-" * 60)
    print(context)

    # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    print("\nğŸ¤– LLM í”„ë¡¬í”„íŠ¸:")
    print("-" * 60)
    print(prompt[:300] + "...\n")

    # ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œ
    print("   â†’ LLMì— ì „ì†¡ (ì‹¤ì œ êµ¬í˜„ ì‹œ)")


def production_rag_demo():
    """í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ ë°ëª¨"""
    print("\nğŸ­ í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ êµ¬ì¡°")
    print("=" * 60)

    code = '''
class ProductionRAG:
    def __init__(self):
        # Vector Store
        self.vectorstore = Chroma(
            persist_directory="./db",
            embedding_function=OpenAIEmbeddings()
        )

        # Retriever (MMR)
        self.retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 3, "fetch_k": 10}
        )

        # LLM
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash"
        )

        # Memory
        self.memory = ConversationBufferMemory()

    def query(self, question):
        # 1. Retrieve
        docs = self.retriever.get_relevant_documents(question)

        # 2. Format context
        context = self.format_docs(docs)

        # 3. Generate answer
        answer = self.llm.invoke(
            f"Context: {context}\\nQuestion: {question}"
        )

        # 4. Save to memory
        self.memory.save_context(
            {"input": question},
            {"output": answer}
        )

        return {
            "answer": answer,
            "sources": [doc.metadata for doc in docs]
        }

# ì‚¬ìš©
rag = ProductionRAG()
result = rag.query("ì§ˆë¬¸")
print(result["answer"])
'''
    print(code)


def main():
    print("ğŸš€ Vector DB Step 5: RAG í†µí•©\n")
    print("=" * 60)

    # ì§€ì‹ ë² ì´ìŠ¤ ì„¤ì •
    print("\n1. ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•")
    print("-" * 60)
    collection = setup_knowledge_base()
    print(f"   âœ… {collection.count()}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

    # ê¸°ë³¸ ê²€ìƒ‰
    print("\n2. ê¸°ë³¸ ê²€ìƒ‰")
    basic_retrieval(collection, "ë¡œì»¬ì—ì„œ LLM ì‹¤í–‰í•˜ëŠ” ë°©ë²•")

    # í•„í„°ë§ ê²€ìƒ‰
    print("\n3. ë©”íƒ€ë°ì´í„° í•„í„°ë§")
    filtered_retrieval(collection, "í”„ë ˆì„ì›Œí¬", "framework")

    # MMR ê²€ìƒ‰
    print("\n4. ë‹¤ì–‘ì„± ê³ ë ¤ (MMR)")
    mmr_retrieval(collection, "LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ")

    # RAG ì‹¤í–‰
    print("\n5. RAG ì§ˆì˜ì‘ë‹µ")
    rag_with_context(collection, "LLMì„ ëª¨ë‹ˆí„°ë§í•˜ë ¤ë©´?")

    # í”„ë¡œë•ì…˜ ë°ëª¨
    production_rag_demo()

    print("\nâœ… Vector DB íŠœí† ë¦¬ì–¼ ì™„ë£Œ!")
    print("\nğŸ“ í•™ìŠµí•œ ë‚´ìš©:")
    print("   1. Vector DB ê¸°ë³¸ (ChromaDB)")
    print("   2. ë¬¸ì„œ ë¶„í•  ë° ì„ë² ë”©")
    print("   3. RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
    print("   4. í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­ (Pinecone)")
    print("   5. LangChain í†µí•©")

    print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
    print("   - Advanced RAG: Hybrid Search, Reranking")
    print("   - Multi-Modal: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸")
    print("   - Langfuse/LangSmith: ëª¨ë‹ˆí„°ë§")
    print("   - GraphRAG: ì§€ì‹ ê·¸ë˜í”„ ê²€ìƒ‰\n")


if __name__ == "__main__":
    main()
