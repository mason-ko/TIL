"""
Fine-tuning Step 3: QLoRA (ν¨μ¨μ  Fine-tuning)

μ£Όμ: μ‹¤μ  ν•™μµμ€ GPUκ°€ ν•„μ”ν•©λ‹λ‹¤
μ΄ μμ λ” κ°λ… μ„¤λ…μ©μ…λ‹λ‹¤

pip install transformers peft bitsandbytes
"""


def explain_qlo_ra():
    print("=== QLoRA (Quantized LoRA) ===\n")
    print("π’΅ ν•µμ‹¬: 4bit μ–‘μν™” + LoRA\n")

    print("π“ λ©”λ¨λ¦¬ λΉ„κµ:")
    print("   Full Fine-tuning: 70B λ¨λΈ = 280GB")
    print("   LoRA: 70B λ¨λΈ = 40GB")
    print("   QLoRA: 70B λ¨λΈ = 20GB β…\n")

    print("π”§ QLoRA κµ¬μ„± μ”μ†:")
    print("   1. 4bit NormalFloat (NF4) μ–‘μν™”")
    print("   2. Double Quantization")
    print("   3. Paged Optimizers\n")

    print("="*60)

    code = '''
# QLoRA μ„¤μ • μμ‹
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True
)

# λ¨λΈ λ΅λ“ (4bit)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=bnb_config,
    device_map="auto"
)

# LoRA μ¶”κ°€
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(model, lora_config)

# ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈ
trainable_params = sum(
    p.numel() for p in model.parameters() if p.requires_grad
)
print(f"ν•™μµ νλΌλ―Έν„°: {trainable_params:,} (μ „μ²΄μ 0.1%)")
'''

    print("\nπ“ QLoRA μ½”λ“:")
    print(code)

    print("\nβ… μ¥μ :")
    print("   - RTX 3090 (24GB)μ—μ„ 13B λ¨λΈ ν•™μµ κ°€λ¥")
    print("   - λΉ„μ© μ κ°: $500 β†’ $20")
    print("   - ν’μ§: Full FT λ€λΉ„ 97%")

    print("\nβ οΈ λ‹¨μ :")
    print("   - ν•™μµ μ†λ„ μ•½κ°„ λλ¦Ό (1.5x)")
    print("   - μ¶”λ΅  μ‹ λ””μ–‘μν™” ν•„μ”")

    print("\nπ’΅ μ‹¤λ¬΄ κ¶μ¥:")
    print("   - 13B μ΄ν•: LoRA")
    print("   - 13B~70B: QLoRA")
    print("   - 70B μ΄μƒ: QLoRA + λ‹¤μ¤‘ GPU")


if __name__ == "__main__":
    explain_qlo_ra()
    print("\nπ“ λ‹¤μ: step4.py - ν‰κ°€ λ° λΉ„κµ\n")
