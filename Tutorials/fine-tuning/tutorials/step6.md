# Step 6: 고급 기법 (RLHF, DPO)

## RLHF (Reinforcement Learning from Human Feedback)

**인간 피드백으로 모델 정렬**

```python
1. SFT (Supervised Fine-tuning)
2. Reward Model 학습
3. PPO로 정책 최적화
```

## DPO (Direct Preference Optimization)

**더 간단한 방법**

```python
선호도 데이터로 직접 학습
RLHF보다 쉽고 빠름
```

---

**핵심**: 인간 선호도 반영
