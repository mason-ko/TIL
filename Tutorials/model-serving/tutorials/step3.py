"""
Model Serving Step 3: ëª¨ë¸ ë¹„êµ ë° ì„ íƒ

ë‹¤ì–‘í•œ ë¡œì»¬ ëª¨ë¸ ë¹„êµ
"""

from langchain_community.llms import Ollama
import time


def benchmark_model(model_name, question):
    """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
    print(f"\n{model_name}:")

    try:
        llm = Ollama(model=model_name)

        start = time.time()
        response = llm.invoke(question)
        elapsed = time.time() - start

        print(f"  ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"  ì‘ë‹µ: {response[:100]}...")

        return elapsed, len(response)

    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {e}")
        print(f"     ollama pull {model_name}")
        return None, None


def compare_models():
    """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ"""
    print("=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===\n")

    question = "Pythonê³¼ JavaScriptì˜ ì°¨ì´ë¥¼ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜"
    print(f"ì§ˆë¬¸: {question}\n")

    models = [
        "llama3:8b",      # 8B íŒŒë¼ë¯¸í„°
        "mistral:7b",     # 7B íŒŒë¼ë¯¸í„°
        "gemma:7b",       # Google 7B
    ]

    results = {}

    for model in models:
        elapsed, length = benchmark_model(model, question)
        if elapsed:
            results[model] = {"time": elapsed, "length": length}

    print("\n" + "=" * 60)
    print("\nê²°ê³¼ ìš”ì•½:")
    print("\nëª¨ë¸           | ì‹œê°„   | ì‘ë‹µ ê¸¸ì´")
    print("-" * 40)

    for model, data in results.items():
        print(f"{model:15} | {data['time']:.2f}ì´ˆ | {data['length']:4}ì")

    print()


def model_recommendations():
    """ëª¨ë¸ ì¶”ì²œ"""
    print("=== ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ ===\n")

    recommendations = [
        ("llama3:8b", "ë²”ìš©", "ê°€ì¥ ê· í˜•ì¡íŒ ì„±ëŠ¥", "â­â­â­â­â­"),
        ("mistral:7b", "ì½”ë”©", "ì½”ë“œ ìƒì„±ì— ê°•í•¨", "â­â­â­â­"),
        ("gemma:7b", "ê°€ë²¼ì›€", "ë¹ ë¥¸ ì‘ë‹µ", "â­â­â­"),
        ("phi3:mini", "ì´ˆê²½ëŸ‰", "3.8B, CPUë„ ê°€ëŠ¥", "â­â­"),
    ]

    print("ëª¨ë¸          | ìš©ë„   | íŠ¹ì§•                | ì¶”ì²œë„")
    print("-" * 65)

    for model, use, feature, rating in recommendations:
        print(f"{model:13} | {use:6} | {feature:18} | {rating}")

    print("\nğŸ’¡ ê¶Œì¥ ì¡°í•©:")
    print("  - ê°œë°œ/í…ŒìŠ¤íŠ¸: llama3:8b")
    print("  - í”„ë¡œë•ì…˜: llama3:70b (GPU í•„ìš”)")
    print("  - ê²½ëŸ‰í™”: phi3:mini (CPU ê°€ëŠ¥)")
    print()


def hardware_requirements():
    """í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­"""
    print("=== í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ ===\n")

    specs = [
        ("7B ëª¨ë¸", "16GB RAM", "CPU", "ëŠë¦¼"),
        ("7B ëª¨ë¸", "8GB VRAM", "GPU", "ë¹ ë¦„ âœ…"),
        ("13B ëª¨ë¸", "16GB VRAM", "GPU", "ë¹ ë¦„"),
        ("70B ëª¨ë¸", "40GB VRAM x2", "GPU", "ë§¤ìš° ë¹ ë¦„"),
    ]

    print("ëª¨ë¸ í¬ê¸°    | ë©”ëª¨ë¦¬    | ì¥ì¹˜ | ì†ë„")
    print("-" * 50)

    for model, memory, device, speed in specs:
        print(f"{model:12} | {memory:9} | {device:4} | {speed}")

    print("\nâœ… RTX 3090 (24GB) ê¶Œì¥")
    print()


if __name__ == "__main__":
    print("ğŸš€ ë¡œì»¬ ëª¨ë¸ ë¹„êµ ë° ì„ íƒ\n")
    print("=" * 60)

    compare_models()
    print("-" * 60)
    model_recommendations()
    print("-" * 60)
    hardware_requirements()

    print("=" * 60)
    print("\nâœ… ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ ì™„ë£Œ!")
    print("\nğŸ“š ìš”ì•½:")
    print("  - ë²”ìš©: llama3:8b")
    print("  - ì½”ë”©: mistral:7b")
    print("  - ê²½ëŸ‰: phi3:mini")
    print()
