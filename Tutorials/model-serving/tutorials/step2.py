"""
Model Serving Step 2: Ollama API ì„œë²„ í™œìš©

ì‹¤í–‰ ì¤‘: ollama serve
"""

import requests
from langchain_community.llms import Ollama


def example1_rest_api():
    """ì˜ˆì œ 1: REST APIë¡œ ì§ì ‘ í˜¸ì¶œ"""
    print("=== Ollama REST API ===\n")

    url = "http://localhost:11434/api/generate"

    data = {
        "model": "llama3",
        "prompt": "LangGraphë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜",
        "stream": False
    }

    print("ìš”ì²­ ì¤‘...")
    response = requests.post(url, json=data)

    if response.status_code == 200:
        result = response.json()
        print(f"ì‘ë‹µ: {result['response']}\n")
    else:
        print(f"âŒ ì˜¤ë¥˜: {response.status_code}")
        print("   ollama serve ì‹¤í–‰ í™•ì¸\n")


def example2_chat_api():
    """ì˜ˆì œ 2: Chat API (ëŒ€í™”í˜•)"""
    print("=== Chat API ===\n")

    url = "http://localhost:11434/api/chat"

    messages = [
        {"role": "user", "content": "ì•ˆë…•! ë„ˆëŠ” ëˆ„êµ¬ì•¼?"},
    ]

    data = {
        "model": "llama3",
        "messages": messages,
        "stream": False
    }

    response = requests.post(url, json=data)

    if response.status_code == 200:
        result = response.json()
        print(f"ì‘ë‹µ: {result['message']['content']}\n")


def example3_streaming():
    """ì˜ˆì œ 3: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    print("=== ìŠ¤íŠ¸ë¦¬ë° (ì‹¤ì‹œê°„ ì¶œë ¥) ===\n")

    llm = Ollama(model="llama3")

    question = "Ollamaì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì¤˜"
    print(f"ì§ˆë¬¸: {question}\n")
    print("ì‘ë‹µ: ", end="", flush=True)

    for chunk in llm.stream(question):
        print(chunk, end="", flush=True)

    print("\n")


def example4_model_management():
    """ì˜ˆì œ 4: ëª¨ë¸ ê´€ë¦¬"""
    print("=== ëª¨ë¸ ê´€ë¦¬ ===\n")

    # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
    response = requests.get("http://localhost:11434/api/tags")

    if response.status_code == 200:
        models = response.json()
        print("ì„¤ì¹˜ëœ ëª¨ë¸:")
        for model in models.get('models', []):
            name = model['name']
            size = model['size'] / (1024**3)  # GBë¡œ ë³€í™˜
            print(f"  - {name} ({size:.1f} GB)")

    print("\nìƒˆ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:")
    print("  ollama pull mistral")
    print("  ollama pull gemma:7b")
    print()


if __name__ == "__main__":
    print("ğŸš€ Ollama API ì„œë²„ í™œìš©\n")
    print("=" * 60)

    try:
        example1_rest_api()
        print("-" * 60)
        example2_chat_api()
        print("-" * 60)
        example3_streaming()
        print("-" * 60)
        example4_model_management()

        print("=" * 60)
        print("\nâœ… Ollama API ì´í•´ ì™„ë£Œ!")
        print("\nğŸ’¡ REST APIë¡œ ì–´ë–¤ ì–¸ì–´ì—ì„œë“  ì‚¬ìš© ê°€ëŠ¥")
        print("ğŸ“š ë‹¤ìŒ: step3.py - ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ\n")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        print("\nOllama ì„œë²„ ì‹¤í–‰ í™•ì¸:")
        print("  ollama serve")
        print("  ollama pull llama3\n")
