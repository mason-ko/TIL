"""
MLOps Step 2: Docker ì»¨í…Œì´ë„ˆí™”

ì‹¤í–‰: docker build -t llm-api .
"""


def create_dockerfile():
    """Dockerfile ìƒì„±"""
    dockerfile_content = '''FROM python:3.11-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì•± ì½”ë“œ ë³µì‚¬
COPY . .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì‹¤í–‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
'''

    print("=== Dockerfile ===\n")
    print(dockerfile_content)

    with open("Dockerfile", "w") as f:
        f.write(dockerfile_content)

    print("âœ… Dockerfile ìƒì„± ì™„ë£Œ\n")


def create_requirements():
    """requirements.txt ìƒì„±"""
    requirements = '''fastapi==0.109.0
uvicorn==0.27.0
langchain-community==0.0.38
python-dotenv==1.0.0
'''

    print("=== requirements.txt ===\n")
    print(requirements)

    with open("requirements.txt", "w") as f:
        f.write(requirements)

    print("âœ… requirements.txt ìƒì„± ì™„ë£Œ\n")


def create_main_app():
    """main.py ìƒì„±"""
    main_content = '''from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="LLM API")

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str

@app.get("/")
def root():
    return {"message": "LLM API Server", "status": "running"}

@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    # OllamaëŠ” Docker ì™¸ë¶€ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ê³  ê°€ì •
    return ChatResponse(response=f"Echo: {request.message}")

@app.get("/health")
def health():
    return {"status": "healthy"}
'''

    print("=== main.py ===\n")
    print(main_content)

    with open("main.py", "w") as f:
        f.write(main_content)

    print("âœ… main.py ìƒì„± ì™„ë£Œ\n")


def docker_commands():
    """Docker ëª…ë ¹ì–´ ê°€ì´ë“œ"""
    print("=== Docker ë¹Œë“œ ë° ì‹¤í–‰ ===\n")

    commands = [
        ("ë¹Œë“œ", "docker build -t llm-api ."),
        ("ì‹¤í–‰", "docker run -p 8000:8000 llm-api"),
        ("ë°±ê·¸ë¼ìš´ë“œ", "docker run -d -p 8000:8000 llm-api"),
        ("ë¡œê·¸ í™•ì¸", "docker logs <container_id>"),
        ("ì¤‘ì§€", "docker stop <container_id>"),
        ("ì´ë¯¸ì§€ ëª©ë¡", "docker images"),
        ("ì»¨í…Œì´ë„ˆ ëª©ë¡", "docker ps"),
    ]

    for desc, cmd in commands:
        print(f"{desc:15} | {cmd}")

    print("\ní…ŒìŠ¤íŠ¸:")
    print("  curl http://localhost:8000")
    print('  curl -X POST http://localhost:8000/chat -H "Content-Type: application/json" -d \'{"message":"Hello"}\'')
    print()


if __name__ == "__main__":
    print("ğŸš€ Docker ì»¨í…Œì´ë„ˆí™”\n")
    print("=" * 60)

    create_dockerfile()
    print("-" * 60)
    create_requirements()
    print("-" * 60)
    create_main_app()
    print("-" * 60)
    docker_commands()

    print("=" * 60)
    print("\nâœ… Docker ì„¤ì • ì™„ë£Œ!")
    print("\nğŸ“¦ ìƒì„±ëœ íŒŒì¼:")
    print("  - Dockerfile")
    print("  - requirements.txt")
    print("  - main.py")
    print("\nğŸš€ ì‹¤í–‰:")
    print("  docker build -t llm-api .")
    print("  docker run -p 8000:8000 llm-api")
    print()
